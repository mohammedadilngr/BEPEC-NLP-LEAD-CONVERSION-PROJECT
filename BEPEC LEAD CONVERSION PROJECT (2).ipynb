{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66d0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac8e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "data=pd.read_excel('1000 leads.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482304d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['Location'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5c53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Status '].replace(to_replace=\"NOt Converted\",value=\"Not Converted\",inplace=True)\n",
    "data['Status '].replace(to_replace='Converted ',value=\"Converted\",inplace=True)\n",
    "data['Status '].replace(to_replace='Conveted',value=\"Converted\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f8c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 4','Lead Name'],inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb1ecec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location              0\n",
       "Status                0\n",
       "Status information    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61562093",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a9b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ff1778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\mohammed\n",
      "[nltk_data]     adil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\mohammed\n",
      "[nltk_data]     adil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['wordnet','stopwords'] )\n",
    "\n",
    "  \n",
    "#ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d3d1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',data['Status information'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in stopwords.words('english')] \n",
    "    # removing stop words and also lematizing\n",
    "    review=\" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6d08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0 ,len(corpus)):\n",
    "    if \"prema\" in corpus[i]:\n",
    "        corpus[i]=corpus[i].replace(\"prema\",'',True)\n",
    "    elif \"mohan\" in corpus[i]:\n",
    "        corpus[i]=corpus[i].replace(\"mohan\",'',True)\n",
    "    elif \"gowtham\" in corpus[i]:\n",
    "        corpus[i]=corpus[i].replace(\"gowtham\",'',True)\n",
    "    elif \"surendra\" in corpus[i]:\n",
    "        corpus[i]=corpus[i].replace(\"surendra\",'',True)\n",
    "    elif \"soma\" in corpus[i]:\n",
    "        corpus[i]=corpus[i].replace(\"soma\",'',True)\n",
    "    else:\n",
    "        corpus[i]=corpus[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa03fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer(max_features=2500)\n",
    "#X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "tv = TfidfVectorizer(max_features=3000,ngram_range=(3,6))      \n",
    "X = tv.fit_transform(corpus).toarray()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8399e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.get_dummies(data['Status ']) #onehot coding for label column into y variable\n",
    "# Not converted : 1\n",
    "#converted :0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38141e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y.iloc[:,1].values \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f5c25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1654, 3000), (1654,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smk = SMOTETomek(random_state=1234)\n",
    "x_new, y_new = smk.fit_resample(X,y)\n",
    "x_new.shape, y_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b17b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_new, y_new, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "206f710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8882175226586103\n",
      "[[166  33]\n",
      " [  4 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90       199\n",
      "           1       0.80      0.97      0.87       132\n",
      "\n",
      "    accuracy                           0.89       331\n",
      "   macro avg       0.89      0.90      0.89       331\n",
      "weighted avg       0.90      0.89      0.89       331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel = 'rbf', random_state = 0).fit(X_train, y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817bb7b0",
   "metadata": {},
   "source": [
    "## Task :2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ee61064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\mohammed\n",
      "[nltk_data]     adil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\mohammed\n",
      "[nltk_data]     adil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['wordnet','stopwords'] )\n",
    "\n",
    "  \n",
    "#ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2032ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    review1=re.sub('[^a-zA-Z]',' ',df['Status information'][i])\n",
    "    review1=review1.lower()\n",
    "    review1=review1.split()\n",
    "\n",
    "    review1 = [wordnet.lemmatize(word) for word in review1 if not word in stopwords.words('english')] \n",
    "    # removing stop words and also lematizing\n",
    "    review1=\" \".join(review1)\n",
    "    corpus1.append(review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e2de678",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    review2=re.sub('[^a-zA-Z]',' ',df['Location'][i])\n",
    "    review2=review2.lower()\n",
    "    review2=review2.split()\n",
    "\n",
    "    review2 = [wordnet.lemmatize(word) for word in review2 if not word in stopwords.words('english')] \n",
    "    # removing stop words and also lematizing\n",
    "    review2=\" \".join(review2)\n",
    "    corpus2.append(review2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a4ee8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BE=[]\n",
    "for i in range(0 ,len(corpus1)):\n",
    "    if \"prema\" in corpus1[i]:\n",
    "        BE.append('prema')\n",
    "        corpus1[i]=corpus1[i].replace(\"prema\",'',True)\n",
    "    elif \"mohan\" in corpus1[i]:\n",
    "        BE.append('mohan')\n",
    "        corpus1[i]=corpus1[i].replace(\"mohan\",'',True)\n",
    "    elif \"gowtham\" in corpus1[i]:\n",
    "        BE.append('gowtham')\n",
    "        corpus1[i]=corpus1[i].replace(\"gowtham\",'',True)\n",
    "    elif \"surendra\" in corpus1[i]:\n",
    "        BE.append('surendra')\n",
    "        corpus1[i]=corpus1[i].replace(\"surendra\",'',True)\n",
    "    elif \"soma\" in corpus1[i]:\n",
    "        BE.append('soma')\n",
    "        corpus1[i]=corpus1[i].replace(\"soma\",'',True)\n",
    "    else:\n",
    "        BE.append('others')\n",
    "        corpus1[i]=corpus1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06046a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BE)\n",
    "df['BE']=BE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91cbb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus3 = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    review3=re.sub('[^a-zA-Z]',' ',df['BE'][i])\n",
    "    review3=review3.lower()\n",
    "    review3=review3.split()\n",
    "\n",
    "    review3 = [wordnet.lemmatize(word) for word in review3 if not word in stopwords.words('english')] \n",
    "    # removing stop words and also lematizing\n",
    "    review3=\" \".join(review3)\n",
    "    corpus3.append(review3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aca1e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer(max_features=2500)\n",
    "#X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "tv1 = TfidfVectorizer(max_features=3000,ngram_range=(3,6))      #defining tfidf vec for transformation\n",
    "x1 = tv1.fit_transform(corpus1).toarray()#transforming our preprocesssed data to vectors adn array\n",
    "tv2 = TfidfVectorizer(max_features=3000,ngram_range=(1,2))      #defining tfidf vec for transformation\n",
    "x2 = tv2.fit_transform(corpus2).toarray()\n",
    "tv3 = TfidfVectorizer(max_features=3000,ngram_range=(1,2))\n",
    "x3 = tv3.fit_transform(corpus3).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77c836e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=pd.get_dummies(df['Status ']) #onehot coding for label column into y variable\n",
    "# Not converted : 1\n",
    "#converted :0\n",
    "y=y.iloc[:,1].values \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f60886a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.concatenate((x1,x2,x3),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73c54f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1658, 3051), (1658,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smk = SMOTETomek(random_state=1234)\n",
    "x, y = smk.fit_resample(x,y)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03276268",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54011aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9066265060240963\n",
      "[[156  24]\n",
      " [  7 145]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91       180\n",
      "           1       0.86      0.95      0.90       152\n",
      "\n",
      "    accuracy                           0.91       332\n",
      "   macro avg       0.91      0.91      0.91       332\n",
      "weighted avg       0.91      0.91      0.91       332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel = 'rbf', random_state = 0).fit(X_train, y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
